\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
	\addtolength{\oddsidemargin}{-1in}
	\addtolength{\evensidemargin}{-1in}
	\addtolength{\textwidth}{1.9in}

	\addtolength{\topmargin}{-1in}
	\addtolength{\textheight}{1in}
\usepackage{fullpage}

\begin{document}

\title{Online EM Project\\ Statistics 221
}
\author{Hannah Director, Kevin Eskici, Willy Xiao}

\maketitle

\section{Literature Review}
The \textbf{\it{Online Em Algorithm for Latent Data Models}} proposed by Cappe and Moulines seeks to define a more efficient EM algorithm for latent data models by allowing for updates that use only data from the current iteration rather than from the entire data set. For big data problems, this is highly beneficial, since storing and retrieving all the data in these types of problems is not computationally feasible. As in traditional EM, the goal of online EM remains to find the maximum likelihood estimates for a given model when the intractability of the integrals renders direct calculation of these estimates impossible. This approach is often needed because of latent variables. In this paper, Cappe and Moulines focus on this use of EM for latent variables and extend it for online use.\\

We now lay out their approach. Like in batch EM, the maximum likelihood estimates for the parameters are obtained by iterating between the E-step and the M-step. In the E-step, the expectation of the log-likelihood with respect to the latent variables given the data is found keeping the estimates of the parameters from the previous iteration as constant. In the M-step, the values of the parameters are found such that they maximize this expectation, referred to as $Q(\theta | \theta^{(t)})$. In the proposed online approach, the M-step is unchanged, but the E-step is replaced by a a stochastic approximation step. Specifically, the authors use the form of the stochastic gradient descent (sgd) for updates on $Q(\theta | \theta^{(t)})$, giving
\begin{equation}
\hat{Q}_{n+1}(\theta) = \hat{Q}_{n}(\theta) + \gamma_{n+1}(\mathbb{E}_{\hat{\theta}_{n}}[log f(X_{n+1};\theta)|Y_{n+1}] - \hat{Q}_{n}(\theta))
\end{equation}
where $\gamma_{n+1}$ corresponds to the learning rate. Assuming the complete data likelihood is from the exponential family and that there is a unique global maximum of the parameters which is a function of the sufficient statistics, $\overline{\theta}(s)$,  as well as some other regularity conditions, the E-step reduces to just finding the expectations of the complete data sufficient statistics using these sgd updates. Then the entire online EM algorithm can be summarize quite simply as follows
\begin{align}
\hat{s}_{n+1} &= \hat{s}_{n} + \gamma_{n+1}(\overline{s}(Y_{n+1}; \hat{\theta}_{n}) - \hat{s}_{n}) \nonumber \\
\hat{\theta}_{n+1} &= \overline{\theta}(\hat{s}_{n+1})
\end{align}
These steps are repeated for each $Y_{i}$ until all the data has been cycled through.

The theoretical justification of this approach is largely dependent on the work of Lange (1995). Lange had the insight that the M-step in batch EM can be replaced by a Newton-Raphson step or a stochastic approximation of the Newton steps (under some conditions) giving a recursion for the parameter updates that is "locally equivalent to the EM algorithm at convergence." The corresponding proposed updates are referred to as the online gradient algorithm. Cappe and Mouline show that their online updates can be manipulated to be of the same form as the online gradient algorithm when approaching convergence, so their algorithm has the same convergence performance. 




\section{Tasks}
\subsection{Implementation 1}

\subsection{Implementation 2}

\newpage
\subsection{Novelty}
In this section, we apply the online EM method presented by Cappe and Moulines to the network traffic model proposed by Cao et al. However, applying online EM directly to this problem is not feasible, because the goal of the network model analysis is to try and determine changes in the mean overtime of  the number of messages going to and from the various router combinations.  Since at each time point, there is only one count of incoming and outgoing messages, there is only one data point for each time period. In the work of Cao et al., each data point is combined with its surrounding data points to give a window of 15 points to estimate the mean for each time period using EM. Online EM only makes one pass through the data, so in this case each time period's means would be estimated with only 15 iterations of the algorithm. This is unlikely to converge, which is demonstrated by figure X which shows that the time series estimated by online EM do not match those estimated by traditional EM.  (ADD FIGURE AND CONFIRM THAT THIS IS THE CASE)\\

So, instead, we augment the data in each window. We do this, by finding the sample mean and variance in each of the windows. Then we draw X (NEED NUMBER) new data points from a normal distribution with mean equal to the observed sample mean and variance equal to observed sample variance. The distribution is truncated such that all values are greater than 0 and selected value are rounded to integers. (We considered the use of a true count model for drawing the augmented data; however, we were not able to find a suitable model. The observed mean and variance were not aproximately equal, making a Poisson model inappropriate and the limited data available did not allow the use of more parameterized models such as the Negative Binomial) Combining the augmented data for each window with the observed data, we then apply the online EM approach posed by Cappe and Moulines to each window. Specifically, we iterated between finding the sufficient statistics $m_{t}$ and $R^{(k)}$ using only the data point from the current iteration and maximizing $Q(\theta | \theta^{(t)})$ where $m_{t}$ and $R^{(k)}$ and  $Q(\theta | \theta^{(t)})$ are as given in the work of Cao et al.\\


From this analysis, we build the following time series. We conclude (THINGS) \\\


From this analysis, we demonstrate that online EM is not appropriate for small data sets, since the algorithm is unlikely to converge. This work also suggests that applying a data augmentation strategy and then applying online EM may be a feasible way to address small data sets that are converging very slowly with typical EM. Potentially, one could augment their data and then  Nevertheless, significant theoretical work would need to be done to determine in what scenarios this method is appropriate and whether such an approach is actually more efficient.

\newpage
\section{Conclusion}
The online EM approach proposed by Cappe and Moulines can be a useful and efficient alternative to traditional EM in certain cases. The approach is designed to solve problems that are known to have latent variables and for which a traditional EM approach will break down. The most obvious example of this situation is for large data sets, since it becomes impossible to store and transfer all the data at once. In this situation, online EM is very useful in that it allows a practictioner to use only one data point at a time, rather than to store all the results. \\

Another situation where this online EM method is particularly useful is cases where the true model of the data is unknown. Most online methods make use of the complete data information matrix of the model of the data. This does not present a theoretical problem when there is high confidence that the proposed model is correct. However, when the model is not correct, many of the theoretical gurantees given by other online methods are no longer valid. This makes the application of these models questionable. On the other hand, the online EM method proposed by Cappe and Moullines does not use the complete data information matrix. Thus, this method is likely to be much more robust to a misspecification of the model. \\

Despite these situations where the proposed online EM method is valuable, there are several cases where this approach breaks down. The most obvious of these cases is applying this method to relatively small data sets. In the proposed online method, there is only one pass through the data. This means that for small data sets, there are certainly not enough iterations through the algorithm to approach convergence, especially given that less information is being used on each iteration than is used in traditional batch EM. Thus, although applying the online EM algorithm may seem like an appealing option for overcoming a traditional EM implementation that is slow to convergene, for small data sets this approach is ill-advised. Instead, the algorithm will finish quickly, but convergence to the true maximum likelihood parameter estimates is unlikely to be obtained.\\

Another situation in which this approach does not perform well is for models where very little is know about the parameters a priori. In these cases, the online EM algorithm can easily break down because of poorly specified initial values. For example, in the Poisson mixture model, if little is know about the initial parameters, one might set all the Poisson means to be relatively low, even though the true means could be quite high. In such a case, if a particularly high data value is observed early in the implementation, the probability of observing such a value could be numerically zero, which causes the algorithm to break down. This issue can likely be overcome by implementing an "initialization period" where the first N data points would simply be collected and used to form initial paramter estimates. However, additional work will be needed to determine the best practices for doing this, such as determining the length of N in different scenarios and how much information loss occurs as a byproduct of using an initialization period. \\

In short, the online EM algorithm for latent data purported by Cappe and Moullines can be a valuable tool for more efficient implementation of the EM algorithm for large data sets. However, it should not be applied to small data sets and caution should be used when there is little prior information available about reasoning initial values for the parameters.

\end{document}

