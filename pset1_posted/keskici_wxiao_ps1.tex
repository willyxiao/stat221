\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage{enumerate}
\usepackage{amsmath}
\newcommand{\parens}[1]{ \left( #1 \right) }
\begin{document}
\begin{enumerate}[1.]
  \item
    \begin{enumerate}[1.]
      \item
        % We know $\vec{x} \sim N_d(\vec{\mu},\Sigma)$ and the density defined as $f_{\vec{x}}(\vec{x})$. \\
        The density of $\vec{u}$, is $f_{\vec{u}} = f_{\vec{x}}(g^{-1}(\vec{u}))||J||$ \\

        First, we find the partials:
        \begin{align*}
          \frac{\partial x_i}{\partial u_j} &= \frac{1}{u_{d+1}} \\
          \frac{\partial x_i}{\partial j_i} &= \frac{1}{u_i} + \frac{1}{u_{d+1}}
        \end{align*}
        Which gives us the Jacobian:
        \begin{align*}
          J &=
              \begin{bmatrix}
                \frac{\partial x_1}{\partial u_1} & \ldots & \frac{\partial x_1}{\partial u_d} \\
                \vdots & \ddots & \vdots \\
                \frac{\partial x_d}{\partial u_1} & \ldots & \frac{\partial x_n}{\partial u_d}
              \end{bmatrix} =
              \begin{bmatrix}
                \frac{1}{u_1} + \frac{1}{u_{d+1}} & & \frac{1}{u_{d+1}} \\
                & \ddots & \\
                \frac{1}{u_{d+1}} & & \frac{1}{u_d} + \frac{1}{u_{d+1}} \\
              \end{bmatrix} \\ \\
            &=
              \begin{bmatrix}
                \frac{1}{u_1} \\
                \vdots \\
                \frac{1}{u_d} \\
              \end{bmatrix}I_d +
              \begin{bmatrix}
                \vdots \\
                \frac{1}{u_{d+1}} \\
                \vdots
              \end{bmatrix}
        \end{align*}
        Letting $X =  \begin{bmatrix}
                        \frac{1}{u_1} \\
                        \vdots \\
                        \frac{1}{u_d} \\
                      \end{bmatrix}$
        and $C =  \begin{bmatrix}
                    \vdots \\
                    \frac{1}{u_{d+1}} \\
                    \vdots
                  \end{bmatrix}$
        , Sylvester's Formula gives us that:
        \begin{align*}
          \mbox{det(} J \mbox{)}
              &=  \mbox{det(} X\times I_d \mbox{)} \cdot (1 + CX^{-1}\begin{bmatrix} \ldots & 1 & \ldots \end{bmatrix}) \\
              &=  \parens{ \prod_{i = 1}^{d}{\frac{1}{u_{i}}} }
                  \parens{ 1 + \parens{ \frac{\frac{1}{u_{d+1}}}{\frac{1}{u_1}} + \ldots + \frac{\frac{1}{u_{d+1}}}{\frac{1}{u_d}} } } \\
              &=  \parens{ \prod_{i = 1}^{d}{\frac{1}{u_{i}}} }
                  \parens{ \frac{u_1 + \ldots + u_d}{u_{d+1}} + \frac{u_{d+1}}{u_{d+1}} } \\
              &=  \parens{ \prod_{i = 1}^{d}{\frac{1}{u_{i}}} }
                  \parens{ \frac{1 - u_{d+1}}{u_{d+1}} + \frac{u_{d+1}}{u_{d+1}}} \\
              &=  \parens{ \prod_{i = 1}^{d+1}{u_{i}} }^{-1}
        \end{align*}
        Thus, $||J|| = \parens{ \prod_{i = 1}^{d+1}{u_{i}} }^{-1}$. We need not worry about the absolute value as each element $u_i \in [0,1]$. \\

        So we know the density of $\vec{u}$ to be (for $\vec{u} \in S^d$):
        \begin{align*}
          f_{\vec{u}}(\vec{u}) = |2\pi \Sigma|^{-1/2}
                        \parens{ \prod_{i = 1}^{d+1}{u_{i}} }^{-1}
                        \mbox{exp}
                          \left[
                            -\frac{1}{2}
                            \left\{
                              log\parens{ \frac{u}{u_{d+1}}}-\mu \right\}^T\Sigma^{-1}\left\{log\parens{ \frac{u}{u_{d+1}} }-\mu
                            \right\}
                          \right]
        \end{align*}
      \item Finding the MLEs: \\
        First for $\vec{\mu}$, we find $\frac{\partial{f_{\vec{u}}(\vec{u})}}{\partial{\vec{\mu}}} = 0$, which becomes: \\
        \begin{align*}
          0 &= \frac{\partial{(-\frac{1}{2}
                            \left\{
                              log\parens{ \frac{u}{u_{d+1}}}-\mu \right\}^T\Sigma^{-1}\left\{log\parens{ \frac{u}{u_{d+1}} }-\mu
                            \right\}
                )}}{\partial{\vec{\mu}}} \\
            &= -\frac{1}{2}\parens{
              -2\Sigma^{-1}\parens{
                log\parens{ \frac{u}{ u_{d+1} } } - \mu
              }
            } \\
          \mu &= log\parens{ \frac{u}{ u_{d+1} } } \\
          \hat{\vec{\mu}}_{mle} &= \frac{\sum_{i=1}^n{log \parens{ \frac{\vec{u}_i}{(u_{d+1})_i}}}}{n}
        \end{align*}
        This makes a lot of sense intuitively. The MLE of $\vec{\mu}$, which is the mean of each $u$, is just the emperical mean of each $u$ over all of our observations. \\ \\
        Rather than solving for $\alpha$ and $\beta$ analytically - we came to a solution numerically. $\alpha$ is the diagonal of the covariance matrix, so we just found the mean of the variance terms of each dimension. So we have that: \\
        \begin{align*}
          \alpha &= \frac{\sum_{i=1}^{d}{\frac{1}{n}\sum_{j=1}^{n}{(X_{ij} - \hat{\mu_i})}}}{d}
        \end{align*}
        Similarly, $-\beta$ is the mean of the sample covariance between each pair of dimensions:
        \begin{align*}
          -\beta &= \frac{
            \sum_{i=1}^{d}{
              \sum_{j=i+1}^{d}{
                \mbox{ Cov}(col_i, col_j) }
            }
          }{
            \binom{d}{2}
          } \\
          & \mbox{ where } col_i \mbox{ is the column of data in dimension } i
        \end{align*}
        We coded up the numerical solution and compared it to generated data (in attached R)
    \end{enumerate}
  \item
    \begin{enumerate}[1]
      \item The likelihood function is
        \begin{align*}
          L(g, \theta_L, \theta_H) &= \prod_{i = 1}^{N}{p(y_{i1},\ldots,y_{iP} | g, \theta_L, \theta_H)}
        \end{align*}
        where $N \text{ and } P$ are defined from the data generating process (number of observations and number of features, respectively). Then,
        \begin{align*}
          & p(y_{i1},\ldots,y_{iP} | g, \theta_L, \theta_H) = \prod_{j = i}^{P}{p(y_{ij} | g, \theta_L, \theta_H)} \\
          & p(y_{ij} | g, \theta_L, \theta_H) = g_{H,i} \prod_{k = 1}^{d_j}{(\theta_{H,jk})^{I(y_{ij} = k)}} + g_{L,i} \prod_{k = 1}^{d_j}{(\theta_{L,jk})^{I(y_{ij} = k)}}
        \end{align*}
        where $d_j$ is the number of categories of feature $j$. So we can write the entire likelihood to be: 
        \begin{align*}
          L(g, \theta_L, \theta_H) &=
            \prod_{i = 1}^{N}{\prod_{j = 1}^{P}{\parens{
                g_{H,i} \prod_{k = 1}^{d_j}{(\theta_{H,jk})^{I(y_{ij} = k)}} + g_{L,i} \prod_{k = 1}^{d_j}{(\theta_{L,jk})^{I(y_{ij} = k)}}
            }}} \\
          \text{ and the log-likelihood: } \\
          l(g, \theta_L, \theta_H) &=
            \sum_{i = 1}^{N} \sum_{j=1}^{P} \parens{\log{ \parens{
              g_{H,i} \prod_{k = 1}^{d_j}{(\theta_{H,jk})^{I(y_{ij} = k)}} + g_{L,i} \prod_{k = 1}^{d_j}{(\theta_{L,jk})^{I(y_{ij} = k)}} 
            }}}
        \end{align*}
    \end{enumerate}
\end{enumerate}
\end{document}