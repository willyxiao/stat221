\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage{enumerate}
\usepackage{amsmath}
\newcommand{\parens}[1]{ \left( #1 \right) }
\begin{document}
\begin{enumerate}[1.]
  \item
    \begin{enumerate}[1]
      \item
        % We know $\vec{x} \sim N_d(\vec{\mu},\Sigma)$ and the density defined as $f_{\vec{x}}(\vec{x})$. \\
        The density of $\vec{u}$, is $f_{\vec{u}} = f_{\vec{x}}(g^{-1}(\vec{u}))||J||$ \\

        First, we find the partials:
        \begin{align*}
          \frac{\partial x_i}{\partial u_j} &= \frac{1}{u_{d+1}} \\
          \frac{\partial x_i}{\partial j_i} &= \frac{1}{u_i} + \frac{1}{u_{d+1}}
        \end{align*}
        Which gives us the Jacobian:
        \begin{align*}
          J &=
              \begin{bmatrix}
                \frac{\partial x_1}{\partial u_1} & \ldots & \frac{\partial x_1}{\partial u_d} \\
                \vdots & \ddots & \vdots \\
                \frac{\partial x_d}{\partial u_1} & \ldots & \frac{\partial x_n}{\partial u_d}
              \end{bmatrix} =
              \begin{bmatrix}
                \frac{1}{u_1} + \frac{1}{u_{d+1}} & & \frac{1}{u_{d+1}} \\
                & \ddots & \\
                \frac{1}{u_{d+1}} & & \frac{1}{u_d} + \frac{1}{u_{d+1}} \\
              \end{bmatrix} \\ \\
            &=
              \begin{bmatrix}
                \frac{1}{u_1} \\
                \vdots \\
                \frac{1}{u_d} \\
              \end{bmatrix}I_d +
              \begin{bmatrix}
                \vdots \\
                \frac{1}{u_{d+1}} \\
                \vdots
              \end{bmatrix}
        \end{align*}
        Letting $X =  \begin{bmatrix}
                        \frac{1}{u_1} \\
                        \vdots \\
                        \frac{1}{u_d} \\
                      \end{bmatrix}$
        and $C =  \begin{bmatrix}
                    \vdots \\
                    \frac{1}{u_{d+1}} \\
                    \vdots
                  \end{bmatrix}$
        , Sylvester's Formula gives us that:
        \begin{align*}
          \mbox{det(} J \mbox{)}
              &=  \mbox{det(} X\times I_d \mbox{)} \cdot (1 + CX^{-1}\begin{bmatrix} \ldots & 1 & \ldots \end{bmatrix}) \\
              &=  \parens{ \prod_{i = 1}^{d}{\frac{1}{u_{i}}} }
                  \parens{ 1 + \parens{ \frac{\frac{1}{u_{d+1}}}{\frac{1}{u_1}} + \ldots + \frac{\frac{1}{u_{d+1}}}{\frac{1}{u_d}} } } \\
              &=  \parens{ \prod_{i = 1}^{d}{\frac{1}{u_{i}}} }
                  \parens{ \frac{u_1 + \ldots + u_d}{u_{d+1}} + \frac{u_{d+1}}{u_{d+1}} } \\
              &=  \parens{ \prod_{i = 1}^{d}{\frac{1}{u_{i}}} }
                  \parens{ \frac{1 - u_{d+1}}{u_{d+1}} + \frac{u_{d+1}}{u_{d+1}}} \\
              &=  \parens{ \prod_{i = 1}^{d+1}{u_{i}} }^{-1}
        \end{align*}
        Thus, $||J|| = \parens{ \prod_{i = 1}^{d+1}{u_{i}} }^{-1}$. We need not worry about the absolute value as each element $u_i \in [0,1]$. \\

        So we know the density of $\vec{u}$ to be:
        \begin{align*}
          f_{\vec{u}}(\vec{u}) = |2\pi \Sigma|^{-1/2}
                        \parens{ \prod_{i = 1}^{d+1}{u_{i}} }^{-1}
                        \mbox{exp}
                          \left[
                            -\frac{1}{2}
                            \left\{
                              log\parens{ \frac{u}{u_{d+1}}}-\mu \right\}^T\Sigma^{-1}\left\{log\parens{ \frac{u}{u_{d+1}} }-\mu
                            \right\}
                          \right]
        \end{align*}
    \end{enumerate}
  \item
    \begin{enumerate}[1]
      \item The likelihood function is
        \begin{align*}
          L(g, \theta_L, \theta_H) &= \prod_{i = 1}^{N}{p(y_{i1},\ldots,y_{iP} | g, \theta_L, \theta_H)}
        \end{align*}
        where $N \text{ and } P$ are defined from the data generating process (number of observations and number of features, respectively). Then,
        \begin{align*}
          & p(y_{i1},\ldots,y_{iP} | g, \theta_L, \theta_H) = \prod_{j = i}^{P}{p(y_{ij} | g, \theta_L, \theta_H)} \\
          & p(y_{ij} | g, \theta_L, \theta_H) = g_{H,i} \prod_{k = 1}^{d_j}{(\theta_{H,jk})^{I(y_{ij} = k)}} + g_{L,i} \prod_{k = 1}^{d_j}{(\theta_{L,jk})^{I(y_{ij} = k)}}
        \end{align*}
        where $d_j$ is the number of categories of feature $j$. So we can write the entire likelihood to be: 
        \begin{align*}
          L(g, \theta_L, \theta_H) &=
            \prod_{i = 1}^{N}{\prod_{j = 1}^{P}{\parens{
                g_{H,i} \prod_{k = 1}^{d_j}{(\theta_{H,jk})^{I(y_{ij} = k)}} + g_{L,i} \prod_{k = 1}^{d_j}{(\theta_{L,jk})^{I(y_{ij} = k)}}
            }}} \\
          \text{ and the log-likelihood: } \\
          l(g, \theta_L, \theta_H) &=
            \sum_{i = 1}^{N} \sum_{j=1}^{P} \parens{\log{ \parens{ 
              g_{H,i} \prod_{k = 1}^{d_j}{(\theta_{H,jk})^{I(y_{ij} = k)}} + g_{L,i} \prod_{k = 1}^{d_j}{(\theta_{L,jk})^{I(y_{ij} = k)}} 
            }}}
        \end{align*}
    \end{enumerate}
\end{enumerate}
\end{document}